{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing necessary libraries\n",
    "pip install nltk spacy numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading vader_lexicon: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If executing this jupyter notebook using google colab uncomment the below two lines\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Downloading NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')  # Downloading the lexicon for Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initializing SentimentIntensityAnalyzer using nltk.sentiment\n",
    "sentimentIntensityAnalyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Function preprocesses the input string text by converting it to lowercase, tokenizing it,\n",
    "    removing stop words, applying stemming, and joining tokens back into a string and returns the preprocessed text (string).\n",
    "    \"\"\"\n",
    "    # Convert text to lowercase\n",
    "    # text = text.lower()\n",
    "    lowercase_text = text.lower()\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(lowercase_text)\n",
    "\n",
    "    # Removing stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Applying stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Joining tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition (NER) using spaCy\n",
    "naturalLangaugeProcessor = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    \"\"\"\n",
    "    Function extracts named entities from the given text, takes string input and returns list of named entities found in the text.\n",
    "    \"\"\"\n",
    "    nlp_doc = naturalLangaugeProcessor(text)\n",
    "    named_entities = [entity.text for entity in nlp_doc.ents]\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis using NLTK\n",
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Function performs sentiment analysis on the input string text using NLTK's SentimentIntensityAnalyzer \n",
    "    and returns sentiment polarity value (-1 for negative, 0 for neutral, 1 for positive).\n",
    "    \"\"\"\n",
    "    # Analyze sentiment using SentimentIntensityAnalyzer\n",
    "    sentiment_scores = sentimentIntensityAnalyzer.polarity_scores(text)\n",
    "\n",
    "    # Extract compound score\n",
    "    compound_score = sentiment_scores['compound']\n",
    "\n",
    "    # Classify sentiment based on compound score\n",
    "    if compound_score >= 0.05:\n",
    "        return 1  # Positive sentiment\n",
    "    elif compound_score <= -0.05:\n",
    "        return -1  # Negative sentiment\n",
    "    else:\n",
    "        return 0  # Neutral sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embeddings using spaCy\n",
    "def get_word_embeddings(text):\n",
    "    \"\"\"\n",
    "    Function computes word embeddings for the given text (string) using spaCy and return vector representation of the text.\n",
    "    \"\"\"\n",
    "    doc = naturalLangaugeProcessor(text)\n",
    "    return doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If executing this jupyter notebook using google colab uncomment the \"Drive data path\" and comment \"Local data path\", \n",
    "# default path: local.\n",
    "\n",
    "# Drive data path\n",
    "# data_path = \"/content/drive/MyDrive/datasets_coursework1/bbc\"\n",
    "\n",
    "# Local data path\n",
    "data_path = \"../Part 2/bbc\"\n",
    "\n",
    "# Initialize list to store preprocessed data\n",
    "preprocessed_data = []\n",
    "\n",
    "# List of news categories\n",
    "news_categories = ['tech', 'business', 'sport', 'politics', 'entertainment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through news categories and dataset files\n",
    "for category in news_categories:\n",
    "    category_path = os.path.join(data_path, category)\n",
    "    files = os.listdir(category_path)\n",
    "    for file in files:\n",
    "        with open(os.path.join(category_path, file), 'r', encoding='latin-1') as f:\n",
    "            text = f.read()\n",
    "            preprocessed_text = preprocess_text(text)\n",
    "            named_entities = extract_named_entities(text)  # Extracting named entities\n",
    "            sentiments = get_sentiment(text)  # Performing sentiment analysis\n",
    "            word_embeddings = get_word_embeddings(text)  # Computing word embeddings\n",
    "            preprocessed_data.append({\n",
    "                'text': preprocessed_text,\n",
    "                'category': category,\n",
    "                'named_entities': named_entities,\n",
    "                'sentiments': sentiments,\n",
    "                'word_embeddings': word_embeddings\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame from the preprocessed data\n",
    "dataframe = pd.DataFrame(preprocessed_data)\n",
    "\n",
    "# Feature Extraction (TF-IDF) using sklearn.feature_extraction.text\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(dataframe['text'])\n",
    "y = dataframe['category']\n",
    "\n",
    "# Initializing Random Forest classifier from sklearn.ensemble\n",
    "randomForestClassifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initializing feature selector using SelectKBest, chi2 from sklearn.feature_selection\n",
    "feature_selector = SelectKBest(chi2, k=1000)\n",
    "\n",
    "# Initializing stratified k-fold cross-validation using sklearn.model_selection\n",
    "skf_cross_validator = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.94      0.94      0.94        81\n",
      "entertainment       0.95      0.95      0.95        62\n",
      "     politics       0.96      0.97      0.96        67\n",
      "        sport       0.98      0.96      0.97        82\n",
      "         tech       0.95      0.95      0.95        64\n",
      "\n",
      "     accuracy                           0.96       356\n",
      "    macro avg       0.95      0.96      0.96       356\n",
      " weighted avg       0.96      0.96      0.96       356\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.95      0.95      0.95        81\n",
      "entertainment       0.98      0.92      0.95        62\n",
      "     politics       0.94      0.96      0.95        67\n",
      "        sport       0.94      1.00      0.97        82\n",
      "         tech       0.97      0.94      0.95        64\n",
      "\n",
      "     accuracy                           0.96       356\n",
      "    macro avg       0.96      0.95      0.95       356\n",
      " weighted avg       0.96      0.96      0.95       356\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.94      0.95      0.94        81\n",
      "entertainment       1.00      0.95      0.98        62\n",
      "     politics       0.97      0.93      0.95        67\n",
      "        sport       0.96      0.99      0.98        82\n",
      "         tech       0.94      0.98      0.96        64\n",
      "\n",
      "     accuracy                           0.96       356\n",
      "    macro avg       0.96      0.96      0.96       356\n",
      " weighted avg       0.96      0.96      0.96       356\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.96      0.98      0.97        81\n",
      "entertainment       1.00      0.94      0.97        62\n",
      "     politics       0.97      0.97      0.97        67\n",
      "        sport       0.98      1.00      0.99        82\n",
      "         tech       0.97      0.98      0.98        64\n",
      "\n",
      "     accuracy                           0.97       356\n",
      "    macro avg       0.98      0.97      0.97       356\n",
      " weighted avg       0.98      0.97      0.97       356\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.94      0.98      0.96        81\n",
      "entertainment       1.00      0.94      0.97        62\n",
      "     politics       0.94      0.97      0.96        67\n",
      "        sport       0.98      0.99      0.98        82\n",
      "         tech       0.98      0.95      0.97        64\n",
      "\n",
      "     accuracy                           0.97       356\n",
      "    macro avg       0.97      0.96      0.97       356\n",
      " weighted avg       0.97      0.97      0.97       356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cross-Validation with Grid Search\n",
    "\n",
    "# Defining the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Performing cross-validation using StratifiedKFold\n",
    "for train_index, test_index in skf_cross_validator.split(X, y):\n",
    "    # Spliting the data into training and testing sets based on the indices generated by StratifiedKFold\n",
    "    X_train_test, X_test = X[train_index], X[test_index]  # Training and testing data for features (TF-IDF matrix)\n",
    "    y_train_test, y_test = y[train_index], y[test_index]  # Training and testing data for target labels\n",
    "\n",
    "    # Further spliting the training set into train and development sets\n",
    "    for train_index_cv, dev_index in skf_cross_validator.split(X_train_test, y_train_test):\n",
    "        X_train_cv, X_dev_cv = X_train_test[train_index_cv], X_train_test[dev_index]\n",
    "        y_train_cv, y_dev_cv = y_train_test.iloc[train_index_cv], y_train_test.iloc[dev_index]\n",
    "\n",
    "        # Initializing a new feature selector for each fold\n",
    "        selector_inner = SelectKBest(chi2, k=1000)\n",
    "\n",
    "        # Performing feature selection on the training set\n",
    "        X_train_selected = selector_inner.fit_transform(X_train_cv, y_train_cv)\n",
    "        X_dev_selected = selector_inner.transform(X_dev_cv)\n",
    "\n",
    "        # Performing grid search to find the best hyperparameters\n",
    "        grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3)\n",
    "        grid_search.fit(X_train_selected, y_train_cv)\n",
    "        best_model_inner = grid_search.best_estimator_\n",
    "\n",
    "        # Evaluating the best model on the development set\n",
    "        y_pred_dev = best_model_inner.predict(X_dev_selected)\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_dev_cv, y_pred_dev))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
